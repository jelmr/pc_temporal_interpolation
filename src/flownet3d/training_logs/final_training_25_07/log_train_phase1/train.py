'''
    Single-GPU training code
'''

import argparse
from datetime import datetime
import numpy as np
import tensorflow as tf
import importlib
import os
import sys
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(BASE_DIR)
import synthetic_dataset

parser = argparse.ArgumentParser()
parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')
parser.add_argument('--model', default='model_concat_upsa', help='Model name [default: model_concat_upsa]')
# parser.add_argument('--data', default='/home/jelmer/data/with_normals/data_0.pickle', help='Dataset directory [default: data_preprocessing/data_processed_maxcut_35_20k_2k_8192]')
#parser.add_argument('--train_data', default='/home/jelmer/torch/data/syntheticadvanced/processed/sf_stable_train_4.pickle', help='dataset directory [default: data_preprocessing/data_processed_maxcut_35_20k_2k_8192]')
#parser.add_argument('--test_data', default='/home/jelmer/torch/data/syntheticadvanced/processed/sf_stable_test_4.pickle', help='dataset directory [default: data_preprocessing/data_processed_maxcut_35_20k_2k_8192]')
parser.add_argument('--train_data', default='/home/jelmer/torch/data/syntheticadvanced_fullframes/processed/sf_stable_train.pickle', help='dataset directory [default: data_preprocessing/data_processed_maxcut_35_20k_2k_8192]')
parser.add_argument('--test_data', default='/home/jelmer/torch/data/syntheticadvanced_fullframes/processed/sf_stable_test.pickle', help='dataset directory [default: data_preprocessing/data_processed_maxcut_35_20k_2k_8192]')
parser.add_argument('--log_dir', default='log_train', help='Log dir [default: log_train]')
parser.add_argument('--num_point', type=int, default=2048, help='Point Number [default: 2048]')
parser.add_argument('--max_epoch', type=int, default=151, help='Epoch to run [default: 151]')
parser.add_argument('--batch_size', type=int, default=8, help='Batch Size during training [default: 16]')
parser.add_argument('--learning_rate', type=float, default=0.002, help='Initial learning rate [default: 0.001]')
parser.add_argument('--momentum', type=float, default=0.9, help='Initial learning rate [default: 0.9]')
parser.add_argument('--optimizer', default='adam', help='adam or momentum [default: adam]')
parser.add_argument('--decay_step', type=int, default=60000, help='Decay step for lr decay [default: 200000]')
parser.add_argument('--decay_rate', type=float, default=0.80, help='Decay rate for lr decay [default: 0.7]')
FLAGS = parser.parse_args()

os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu)

EPOCH_CNT = 0

BATCH_SIZE = FLAGS.batch_size
NUM_POINT = FLAGS.num_point
TRAIN_DATA = FLAGS.train_data
TEST_DATA = FLAGS.test_data
MAX_EPOCH = FLAGS.max_epoch
BASE_LEARNING_RATE = FLAGS.learning_rate
GPU_INDEX = FLAGS.gpu
MOMENTUM = FLAGS.momentum
OPTIMIZER = FLAGS.optimizer
DECAY_STEP = FLAGS.decay_step
DECAY_RATE = FLAGS.decay_rate

MODEL = importlib.import_module(FLAGS.model) # import network module
MODEL_FILE = os.path.join(BASE_DIR, FLAGS.model+'.py')
LOG_DIR = FLAGS.log_dir
if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)
os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def
os.system('cp %s %s' % (__file__, LOG_DIR)) # bkp of train procedure
os.system('cp %s %s' % ('flying_things_dataset.py', LOG_DIR)) # bkp of dataset file
LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')
LOG_FOUT.write(str(FLAGS)+'\n')

BN_INIT_DECAY = 0.5
BN_DECAY_DECAY_RATE = 0.5
BN_DECAY_DECAY_STEP = float(DECAY_STEP)
BN_DECAY_CLIP = 0.99

TRAIN_DATASET = synthetic_dataset.SceneflowDataset(TRAIN_DATA, npoints=NUM_POINT)
TEST_DATASET = synthetic_dataset.SceneflowDataset(TEST_DATA, npoints=NUM_POINT, train=False)

def log_string(out_str):
    LOG_FOUT.write(out_str+'\n')
    LOG_FOUT.flush()
    print(out_str)

def get_learning_rate(batch):
    learning_rate = tf.train.exponential_decay(
                        BASE_LEARNING_RATE,  # Base learning rate.
                        batch * BATCH_SIZE,  # Current index into the dataset.
                        DECAY_STEP,          # Decay step.
                        DECAY_RATE,          # Decay rate.
                        staircase=True)
    learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!
    return learning_rate

def get_bn_decay(batch):
    bn_momentum = tf.train.exponential_decay(
                      BN_INIT_DECAY,
                      batch*BATCH_SIZE,
                      BN_DECAY_DECAY_STEP,
                      BN_DECAY_DECAY_RATE,
                      staircase=True)
    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)
    return bn_decay

def train():
    with tf.Graph().as_default():
        with tf.device('/gpu:'+str(GPU_INDEX)):
            pointclouds_pl, labels_pl, masks_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)
            is_training_pl = tf.placeholder(tf.bool, shape=())

            # Note the global_step=batch parameter to minimize.
            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.
            batch = tf.Variable(0)
            bn_decay = get_bn_decay(batch)
            tf.summary.scalar('bn_decay', bn_decay)

            print("--- Get model and loss")
            # Get model and loss
            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)
            loss = MODEL.get_loss(pred, labels_pl, masks_pl, end_points)
            tf.summary.scalar('loss', loss)


            print("--- Get training operator")
            # Get training operator
            learning_rate = get_learning_rate(batch)
            tf.summary.scalar('learning_rate', learning_rate)
            if OPTIMIZER == 'momentum':
                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)
            elif OPTIMIZER == 'adam':
                optimizer = tf.train.AdamOptimizer(learning_rate)
            train_op = optimizer.minimize(loss, global_step=batch)

            # Add ops to save and restore all the variables.
            saver = tf.train.Saver()




        # Create a session
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        config.allow_soft_placement = True
        config.log_device_placement = False
        sess = tf.Session(config=config)

        # Add summary writers
        merged = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)


        epe, acc1, acc2 = scene_flow_EPE_np(pred, labels_pl, masks_pl)
        tf.summary.scalar("epe", epe)
        tf.summary.scalar("acc 5%", acc1)
        tf.summary.scalar("acc 10%", acc2)
        merged_test = tf.summary.merge_all()
        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)



        # Init variables
        init = tf.global_variables_initializer()
        sess.run(init)

        ops = {'pointclouds_pl': pointclouds_pl,
               'labels_pl': labels_pl,
               'masks_pl': masks_pl,
               'is_training_pl': is_training_pl,
               'pred': pred,
               'loss': loss,
               'train_op': train_op,
               'merged': merged,
               'merged_test': merged_test,
               'step': batch,
               'end_points': end_points}

        for epoch in range(MAX_EPOCH):
            log_string('**** EPOCH %03d ****' % (epoch))
            sys.stdout.flush()

            train_one_epoch(sess, ops, train_writer)
            eval_one_epoch(sess, ops, test_writer)

            # Save the variables to disk.
            if epoch % 10 == 0:
                save_path = saver.save(sess, os.path.join(LOG_DIR, "model.ckpt"))
                log_string("Model saved in file: %s" % save_path)


def get_batch(dataset, idxs, start_idx, end_idx):
    bsize = end_idx-start_idx
    batch_data = np.zeros((bsize, NUM_POINT*2, 6))
    batch_label = np.zeros((bsize, NUM_POINT, 6))
    batch_mask = np.ones((bsize, NUM_POINT))
    batch_normals = np.zeros((bsize, NUM_POINT, 3))
    # shuffle idx to change point order (change FPS behavior)
    shuffle_idx = np.arange(NUM_POINT)
    np.random.shuffle(shuffle_idx)
    for i in range(bsize):
        pc1, pc2, pc3, color1, color2, color3, normals1, mask1 = dataset[idxs[i+start_idx]]
        # move pc1 to center
        pc1_center = np.mean(pc1, 0)
        pc1 -= pc1_center
        pc3 -= pc1_center
        batch_data[i,:NUM_POINT,:3] = pc1[shuffle_idx, ...]
        batch_data[i,:NUM_POINT,3:] = color1[shuffle_idx, ...]
        batch_data[i,NUM_POINT:,:3] = pc2[shuffle_idx, ...]
        batch_data[i,NUM_POINT:,3:] = color2[shuffle_idx, ...]
        batch_label[i, :NUM_POINT, :3] = pc2[shuffle_idx, ...]
        batch_label[i, :NUM_POINT, 3:] = color2[shuffle_idx, ...]
        batch_mask[i] = mask1[shuffle_idx, ...]
        batch_normals[i, :NUM_POINT, :3] = normals1[shuffle_idx, ...]
    return batch_data, batch_label, batch_mask, batch_normals

def train_one_epoch(sess, ops, train_writer):
    """ ops: dict mapping from string to tf ops """
    is_training = True

    # Shuffle train samples
    train_idxs = np.arange(0, len(TRAIN_DATASET))
    np.random.shuffle(train_idxs)
    num_batches = len(TRAIN_DATASET) // BATCH_SIZE

    log_string(str(datetime.now()))

    loss_sum = 0
    for batch_idx in range(num_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = (batch_idx+1) * BATCH_SIZE
        batch_data, batch_label, batch_mask, batch_normals = get_batch(TRAIN_DATASET, train_idxs, start_idx, end_idx)


        feed_dict = {ops['pointclouds_pl']: batch_data,
                     ops['labels_pl']: batch_normals,
                     ops['masks_pl']: batch_mask,
                     ops['is_training_pl']: is_training,}
        summary, step, _, loss_val, pred_val, end_points = sess.run([ops['merged'], ops['step'],
            ops['train_op'], ops['loss'], ops['pred'], ops['end_points']], feed_dict=feed_dict)
        train_writer.add_summary(summary, step)
        loss_sum += loss_val

        if (batch_idx+1)%5 == 0:
            log_string(' -- %03d / %03d --' % (batch_idx+1, num_batches))
            log_string('mean loss: %f' % (loss_sum / 10))
            loss_sum = 0

        if (batch_idx+1)%100 == 0:

            pc1_batch = batch_data[:, :NUM_POINT, :]
            pc2_batch = batch_data[:, NUM_POINT:, :]
            import pickle
            d = {'pc1': pc1_batch, "pc2":pc2_batch, "end":end_points}
            f = open("/home/jelmer/data/flownet_results/a.pickle", "wb")
            pickle.dump(d, f)
            print('we done pickle')
            # sys.exit(1)



def scene_flow_EPE_np(pred, labels, mask, means=True):
    sq_err = (pred - labels) ** 2
    error = tf.sqrt(tf.reduce_sum(sq_err, 2) + 1e-20)

    gtflow_len = tf.sqrt(tf.reduce_sum(labels*labels, 2) + 1e-20) # B,N
    acc1_bool = tf.logical_or((error <= 0.005), (error/gtflow_len <= 0.05))
    acc1 = tf.reduce_mean(tf.cast(acc1_bool, tf.float32) , axis=1)

    acc2_bool = tf.logical_or((error <= 0.01), (error/gtflow_len <= 0.1))
    acc2 = tf.reduce_mean(tf.cast(acc2_bool, tf.float32), axis=1)


    EPE = tf.reduce_mean(error, 1)
    if means:
        return tf.reduce_mean(EPE), tf.reduce_mean(acc1), tf.reduce_mean(acc2)
    else:
        return EPE, acc1, acc2

def eval_one_epoch(sess, ops, test_writer):
    """ ops: dict mapping from string to tf ops """
    global EPOCH_CNT

    loss_sum = 0
    loss_sum_l2 = 0

    log_string(str(datetime.now()))
    log_string('---- EPOCH %03d EVALUATION ----'%(EPOCH_CNT))

    is_training = False

    # Shuffle train samples
    train_idxs = np.arange(0, len(TEST_DATASET))
    np.random.shuffle(train_idxs)
    num_batches = len(TEST_DATASET) // BATCH_SIZE

    log_string(str(datetime.now()))

    print(">>> We'll be doing ", num_batches, " batches!")

    loss_sum = 0
    for batch_idx in range(num_batches):
        print("Batch [", batch_idx, "/", num_batches)
        if batch_idx %20==0:
            log_string('%03d/%03d'%(batch_idx, num_batches))
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min(len(TEST_DATASET), (batch_idx+1) * BATCH_SIZE)
        current_batch_size = end_idx-start_idx
        if current_batch_size != BATCH_SIZE:
            continue
        batch_data, batch_label, batch_mask, batch_normals = get_batch(TEST_DATASET, train_idxs, start_idx, end_idx)


        feed_dict = {ops['pointclouds_pl']: batch_data,
                     ops['labels_pl']: batch_normals,
                     ops['masks_pl']: batch_mask,
                     ops['is_training_pl']: is_training,}
        summary, step, loss_val, pred_val = sess.run([ops['merged_test'], ops['step'], ops['loss'], ops['pred']], feed_dict=feed_dict)
        test_writer.add_summary(summary, step)

        tmp = np.sum((pred_val - batch_normals)**2, 2) / 2.0
        loss_val_np = np.mean(batch_mask * tmp)
        loss_sum += loss_val
        loss_sum_l2 += loss_val_np

        log_string('eval mean loss: %f' % (loss_sum / float(len(TEST_DATASET)/BATCH_SIZE)))
        log_string('eval mean loss: %f' % (loss_sum_l2 / float(len(TEST_DATASET)/BATCH_SIZE)))

        EPOCH_CNT += 1



# def eval_one_epoch(sess, ops, test_writer):
#     """ ops: dict mapping from string to tf ops """
#     global EPOCH_CNT
#     is_training = False
#     test_idxs = np.arange(0, len(TEST_DATASET))
#
#     # Test on all data: last batch might be smaller than BATCH_SIZE
#     num_batches = (len(TEST_DATASET)+BATCH_SIZE-1) // BATCH_SIZE
#
#     loss_sum = 0
#     loss_sum_l2 = 0
#
#     log_string(str(datetime.now()))
#     log_string('---- EPOCH %03d EVALUATION ----'%(EPOCH_CNT))
#
#     batch_data = np.zeros((BATCH_SIZE, NUM_POINT*2, 3))
#     batch_label = np.zeros((BATCH_SIZE, NUM_POINT, 3))
#     batch_mask = np.zeros((BATCH_SIZE, NUM_POINT))
#     for batch_idx in range(num_batches):
#         if batch_idx %20==0:
#             log_string('%03d/%03d'%(batch_idx, num_batches))
#         start_idx = batch_idx * BATCH_SIZE
#         end_idx = min(len(TEST_DATASET), (batch_idx+1) * BATCH_SIZE)
#         cur_batch_size = end_idx-start_idx
#         cur_batch_data, cur_batch_label, cur_batch_mask = get_batch(TEST_DATASET, test_idxs, start_idx, end_idx)
#         if cur_batch_size == BATCH_SIZE:
#             batch_data = cur_batch_data
#             batch_label = cur_batch_label
#             batch_mask = cur_batch_mask
#         else:
#             batch_data[0:cur_batch_size] = cur_batch_data
#             batch_label[0:cur_batch_size] = cur_batch_label
#             batch_mask[0:cur_batch_size] = cur_batch_mask
#
#         # ---------------------------------------------------------------------
#         # ---- INFERENCE BELOW ----
#         feed_dict = {ops['pointclouds_pl']: batch_data,
#                      ops['labels_pl']: batch_label,
#                      ops['masks_pl']: batch_mask,
#                      ops['is_training_pl']: is_training}
#         summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],
#             ops['loss'], ops['pred']], feed_dict=feed_dict)
#         test_writer.add_summary(summary, step)
#         # ---- INFERENCE ABOVE ----
#         # ---------------------------------------------------------------------
#
#         tmp = np.sum((pred_val - batch_label)**2, 2) / 2.0
#         loss_val_np = np.mean(batch_mask * tmp)
#         if cur_batch_size==BATCH_SIZE:
#             loss_sum += loss_val
#             loss_sum_l2 += loss_val_np
#
#         # Dump some results
#         if batch_idx == 0:
#             with open('test_results.pkl', 'wb') as fp:
#                 pickle.dump([batch_data, batch_label, pred_val], fp)
#
#     log_string('eval mean loss: %f' % (loss_sum / float(len(TEST_DATASET)/BATCH_SIZE)))
#     log_string('eval mean loss: %f' % (loss_sum_l2 / float(len(TEST_DATASET)/BATCH_SIZE)))
#
#     EPOCH_CNT += 1
#     return loss_sum/float(len(TEST_DATASET)/BATCH_SIZE)


if __name__ == "__main__":
    log_string('pid: %s'%(str(os.getpid())))
    train()
    LOG_FOUT.close()
